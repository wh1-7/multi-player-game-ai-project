# AIå®ç°æŒ‡å—

## ğŸ¯ åŸºç¡€AIç®—æ³•å®ç°æŒ‡å¯¼

æœ¬æŒ‡å—æä¾›äº†å‡ ç§åŸºç¡€AIç®—æ³•çš„è¯¦ç»†å®ç°æ–¹æ³•ï¼Œé€‚åˆåˆå­¦è€…å¾ªåºæ¸è¿›åœ°å­¦ä¹ æ¸¸æˆAIå¼€å‘ã€‚

## ğŸ“š AIç®—æ³•éš¾åº¦ç­‰çº§

### ğŸŸ¢ å…¥é—¨çº§ï¼ˆæ¨èæ–°æ‰‹ï¼‰
1. **æ”¹è¿›éšæœºAI** - åœ¨éšæœºé€‰æ‹©çš„åŸºç¡€ä¸Šæ·»åŠ åŸºæœ¬è§„åˆ™
2. **åŸºäºè§„åˆ™çš„AI** - ä½¿ç”¨if-elseæ¡ä»¶åˆ¤æ–­çš„ç­–ç•¥AI

### ğŸŸ¡ ä¸­çº§
3. **è´ªå¿ƒç®—æ³•AI** - æ¯æ­¥é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ
4. **ç®€å•æœç´¢AI** - ä½¿ç”¨BFSæˆ–DFSè¿›è¡Œè·¯å¾„æœç´¢

### ğŸ”´ è¿›é˜¶çº§ï¼ˆé€‰åšï¼‰
5. **å¯å‘å¼AI** - ç»“åˆå¤šç§å¯å‘å¼å‡½æ•°
6. **å¼ºåŒ–å­¦ä¹ AI** - ä½¿ç”¨Q-learningç­‰æ–¹æ³•
7. **å¤§è¯­è¨€æ¨¡å‹AI** - æ¥å…¥GPTç­‰å¤§æ¨¡å‹

## ğŸ”§ 1. æ”¹è¿›éšæœºAI

### åŸºæœ¬æ€è·¯
- åœ¨éšæœºé€‰æ‹©çš„åŸºç¡€ä¸Šï¼Œè¿‡æ»¤æ‰æ˜æ˜¾ä¸å¥½çš„åŠ¨ä½œ
- æ·»åŠ å®‰å…¨æ€§æ£€æŸ¥
- ç®€å•çš„ä½ç½®åå¥½

### å®ç°æ­¥éª¤

```python
class ImprovedRandomBot(BaseAgent):
    def get_action(self, observation, env):
        valid_actions = env.get_valid_actions()
        
        # 1. è¿‡æ»¤å±é™©åŠ¨ä½œ
        safe_actions = self.filter_dangerous_actions(valid_actions, observation, env)
        
        # 2. é€‰æ‹©åå¥½åŠ¨ä½œ
        preferred_actions = self.get_preferred_actions(safe_actions, observation, env)
        
        # 3. éšæœºé€‰æ‹©
        if preferred_actions:
            return random.choice(preferred_actions)
        elif safe_actions:
            return random.choice(safe_actions)
        else:
            return random.choice(valid_actions)
```

### äº”å­æ£‹ç¤ºä¾‹
```python
def filter_dangerous_actions(self, actions, observation, env):
    """è¿‡æ»¤å±é™©åŠ¨ä½œï¼šé¿å…è®©å¯¹æ‰‹ç«‹å³è·èƒœ"""
    safe_actions = []
    board = observation['board']
    opponent = 3 - self.player_id
    
    for action in actions:
        # æ£€æŸ¥è¿™ä¸ªåŠ¨ä½œæ˜¯å¦ä¼šè®©å¯¹æ‰‹åœ¨ä¸‹ä¸€æ­¥è·èƒœ
        if not self.enables_opponent_win(action, board, opponent):
            safe_actions.append(action)
    
    return safe_actions

def get_preferred_actions(self, actions, observation, env):
    """é€‰æ‹©åå¥½ä½ç½®ï¼šä¸­å¿ƒåŒºåŸŸä¼˜å…ˆ"""
    board = observation['board']
    center = board.shape[0] // 2
    
    # æŒ‰è·ç¦»ä¸­å¿ƒçš„è¿œè¿‘æ’åº
    actions_with_distance = []
    for action in actions:
        row, col = action
        distance = abs(row - center) + abs(col - center)
        actions_with_distance.append((action, distance))
    
    # é€‰æ‹©æœ€è¿‘çš„1/3åŠ¨ä½œ
    actions_with_distance.sort(key=lambda x: x[1])
    num_preferred = max(1, len(actions_with_distance) // 3)
    
    return [action for action, _ in actions_with_distance[:num_preferred]]
```

## ğŸ¯ 2. åŸºäºè§„åˆ™çš„AI

### åŸºæœ¬æ€è·¯
- å®šä¹‰ä¸€ç³»åˆ—ä¼˜å…ˆçº§è§„åˆ™
- æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥è§„åˆ™
- æ‰§è¡Œç¬¬ä¸€ä¸ªåŒ¹é…çš„è§„åˆ™

### è§„åˆ™è®¾è®¡æ¡†æ¶

```python
class RuleBasedAI(BaseAgent):
    def __init__(self):
        super().__init__()
        self.rules = [
            self.rule_win_immediately,      # ä¼˜å…ˆçº§1ï¼šç«‹å³è·èƒœ
            self.rule_block_opponent,       # ä¼˜å…ˆçº§2ï¼šé˜»æ­¢å¯¹æ‰‹è·èƒœ
            self.rule_create_threat,        # ä¼˜å…ˆçº§3ï¼šåˆ›é€ å¨èƒ
            self.rule_improve_position,     # ä¼˜å…ˆçº§4ï¼šæ”¹å–„ä½ç½®
            self.rule_safe_move,           # ä¼˜å…ˆçº§5ï¼šå®‰å…¨ç§»åŠ¨
            self.rule_random              # ä¼˜å…ˆçº§6ï¼šéšæœºé€‰æ‹©
        ]
    
    def get_action(self, observation, env):
        for rule in self.rules:
            action = rule(observation, env)
            if action is not None:
                return action
        
        # å…œåº•ï¼šéšæœºé€‰æ‹©
        return random.choice(env.get_valid_actions())
```

### äº”å­æ£‹è§„åˆ™ç¤ºä¾‹

```python
def rule_win_immediately(self, observation, env):
    """è§„åˆ™1ï¼šå¦‚æœèƒ½è·èƒœï¼Œç«‹å³è·èƒœ"""
    valid_actions = env.get_valid_actions()
    board = observation['board']
    
    for action in valid_actions:
        if self.check_win_after_move(board, action, self.player_id):
            return action
    return None

def rule_block_opponent(self, observation, env):
    """è§„åˆ™2ï¼šé˜»æ­¢å¯¹æ‰‹è·èƒœ"""
    valid_actions = env.get_valid_actions()
    board = observation['board']
    opponent = 3 - self.player_id
    
    for action in valid_actions:
        if self.check_win_after_move(board, action, opponent):
            return action
    return None

def rule_create_threat(self, observation, env):
    """è§„åˆ™3ï¼šåˆ›é€ å¨èƒï¼ˆè¿æˆ3å­æˆ–4å­ï¼‰"""
    valid_actions = env.get_valid_actions()
    board = observation['board']
    
    best_action = None
    max_threat = 0
    
    for action in valid_actions:
        threat_level = self.calculate_threat_level(board, action, self.player_id)
        if threat_level > max_threat:
            max_threat = threat_level
            best_action = action
    
    return best_action if max_threat > 0 else None
```

### è´ªåƒè›‡è§„åˆ™ç¤ºä¾‹

```python
def rule_avoid_collision(self, observation, env):
    """è§„åˆ™1ï¼šé¿å…ç¢°æ’"""
    valid_actions = env.get_valid_actions()
    safe_actions = []
    
    for action in valid_actions:
        if not self.will_collide(observation, action):
            safe_actions.append(action)
    
    return safe_actions if safe_actions else None

def rule_eat_food(self, observation, env):
    """è§„åˆ™2ï¼šæœæœ€è¿‘çš„é£Ÿç‰©ç§»åŠ¨"""
    snake = observation[f'snake{self.player_id}']
    foods = observation['foods']
    
    if not foods:
        return None
    
    head = snake[0]
    nearest_food = min(foods, key=lambda f: self.manhattan_distance(head, f))
    
    return self.get_direction_to_target(head, nearest_food)

def rule_stay_center(self, observation, env):
    """è§„åˆ™3ï¼šä¿æŒåœ¨ä¸­å¿ƒåŒºåŸŸ"""
    snake = observation[f'snake{self.player_id}']
    head = snake[0]
    board_size = observation['board'].shape[0]
    center = (board_size // 2, board_size // 2)
    
    return self.get_direction_to_target(head, center)
```

## ğŸ¯ 3. è´ªå¿ƒç®—æ³•AI

### åŸºæœ¬æ€è·¯
- ä¸ºæ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œè®¡ç®—åˆ†æ•°
- é€‰æ‹©åˆ†æ•°æœ€é«˜çš„åŠ¨ä½œ
- åˆ†æ•°å‡½æ•°æ˜¯å…³é”®

### å®ç°æ¡†æ¶

```python
class GreedyAI(BaseAgent):
    def get_action(self, observation, env):
        valid_actions = env.get_valid_actions()
        best_action = None
        best_score = float('-inf')
        
        for action in valid_actions:
            score = self.evaluate_action(action, observation, env)
            if score > best_score:
                best_score = score
                best_action = action
        
        return best_action
    
    def evaluate_action(self, action, observation, env):
        """è¯„ä¼°å‡½æ•°ï¼šæ ¸å¿ƒåœ¨äºå¦‚ä½•è®¡ç®—åˆ†æ•°"""
        return self.calculate_score(action, observation, env)
```

### è¯„ä¼°å‡½æ•°è®¾è®¡

#### äº”å­æ£‹è¯„ä¼°å‡½æ•°
```python
def calculate_score(self, action, observation, env):
    """äº”å­æ£‹ä½ç½®è¯„ä¼°"""
    board = observation['board']
    row, col = action
    score = 0
    
    # 1. è¿å­å¥–åŠ±
    score += self.count_connections(board, row, col, self.player_id) * 10
    
    # 2. é˜»æ­¢å¯¹æ‰‹å¥–åŠ±
    opponent = 3 - self.player_id
    score += self.count_connections(board, row, col, opponent) * 8
    
    # 3. ä½ç½®å¥–åŠ±ï¼ˆä¸­å¿ƒåŒºåŸŸæ›´å¥½ï¼‰
    center = board.shape[0] // 2
    distance_to_center = abs(row - center) + abs(col - center)
    score += (10 - distance_to_center)
    
    # 4. å¨èƒç­‰çº§
    score += self.calculate_threat_potential(board, row, col, self.player_id) * 5
    
    return score

def count_connections(self, board, row, col, player):
    """è®¡ç®—åœ¨æŸä½ç½®å¯ä»¥å½¢æˆçš„è¿æ¥æ•°"""
    max_connections = 0
    directions = [(0,1), (1,0), (1,1), (1,-1)]
    
    for dr, dc in directions:
        count = 1  # å½“å‰ä½ç½®
        
        # æ­£æ–¹å‘è®¡æ•°
        r, c = row + dr, col + dc
        while (0 <= r < board.shape[0] and 0 <= c < board.shape[1] and 
               board[r, c] == player):
            count += 1
            r += dr
            c += dc
        
        # è´Ÿæ–¹å‘è®¡æ•°
        r, c = row - dr, col - dc
        while (0 <= r < board.shape[0] and 0 <= c < board.shape[1] and 
               board[r, c] == player):
            count += 1
            r -= dr
            c -= dc
        
        max_connections = max(max_connections, count)
    
    return max_connections
```

#### è´ªåƒè›‡è¯„ä¼°å‡½æ•°
```python
def calculate_score(self, action, observation, env):
    """è´ªåƒè›‡åŠ¨ä½œè¯„ä¼°"""
    snake = observation[f'snake{self.player_id}']
    foods = observation['foods']
    head = snake[0]
    
    # æ¨¡æ‹Ÿç§»åŠ¨åçš„æ–°ä½ç½®
    new_head = (head[0] + action[0], head[1] + action[1])
    score = 0
    
    # 1. é£Ÿç‰©è·ç¦»å¥–åŠ±ï¼ˆè·ç¦»è¶Šè¿‘åˆ†æ•°è¶Šé«˜ï¼‰
    if foods:
        min_food_distance = min(self.manhattan_distance(new_head, food) for food in foods)
        score += 100 / (min_food_distance + 1)
    
    # 2. å®‰å…¨æ€§å¥–åŠ±
    if self.is_safe_position(new_head, observation, env):
        score += 50
    else:
        score -= 1000  # å±é™©ä½ç½®ä¸¥é‡æƒ©ç½š
    
    # 3. ç©ºé—´å¥–åŠ±ï¼ˆé¿å…è¢«å›°ï¼‰
    free_space = self.count_free_space_around(new_head, observation)
    score += free_space * 2
    
    # 4. ä¸­å¿ƒä½ç½®å¥–åŠ±
    board_size = observation['board'].shape[0]
    center = board_size // 2
    distance_to_center = abs(new_head[0] - center) + abs(new_head[1] - center)
    score += 10 / (distance_to_center + 1)
    
    return score
```

## ğŸ” 4. ç®€å•æœç´¢AI

### BFSæœç´¢ç¤ºä¾‹

```python
from collections import deque

class SearchAI(BaseAgent):
    def get_action(self, observation, env):
        """ä½¿ç”¨BFSå¯»æ‰¾åˆ°ç›®æ ‡çš„æœ€çŸ­è·¯å¾„"""
        current_pos = self.get_current_position(observation)
        target_pos = self.get_target_position(observation)
        
        path = self.bfs_search(current_pos, target_pos, observation, env)
        
        if path and len(path) > 1:
            next_pos = path[1]
            return self.position_to_action(current_pos, next_pos)
        
        return random.choice(env.get_valid_actions())
    
    def bfs_search(self, start, target, observation, env):
        """BFSæœç´¢ç®—æ³•"""
        queue = deque([(start, [start])])
        visited = {start}
        
        while queue:
            position, path = queue.popleft()
            
            if position == target:
                return path
            
            # æ¢ç´¢é‚»å±…èŠ‚ç‚¹
            for neighbor in self.get_neighbors(position, observation, env):
                if neighbor not in visited and self.is_valid_position(neighbor, observation, env):
                    visited.add(neighbor)
                    queue.append((neighbor, path + [neighbor]))
        
        return []  # æœªæ‰¾åˆ°è·¯å¾„
    
    def get_neighbors(self, position, observation, env):
        """è·å–é‚»å±…ä½ç½®"""
        row, col = position
        neighbors = []
        
        for dr, dc in [(-1,0), (1,0), (0,-1), (0,1)]:  # ä¸Šä¸‹å·¦å³
            new_row, new_col = row + dr, col + dc
            neighbors.append((new_row, new_col))
        
        return neighbors
    
    def is_valid_position(self, position, observation, env):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦æœ‰æ•ˆ"""
        row, col = position
        board = observation['board']
        
        # æ£€æŸ¥è¾¹ç•Œ
        if not (0 <= row < board.shape[0] and 0 <= col < board.shape[1]):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éšœç¢ç‰©
        if board[row, col] != 0:  # 0è¡¨ç¤ºç©ºä½
            return False
        
        return True
```

### A*æœç´¢ç¤ºä¾‹

```python
import heapq

class AStarAI(BaseAgent):
    def a_star_search(self, start, target, observation, env):
        """A*æœç´¢ç®—æ³•"""
        open_set = [(0, start, [start])]
        closed_set = set()
        
        while open_set:
            f_score, position, path = heapq.heappop(open_set)
            
            if position in closed_set:
                continue
            
            closed_set.add(position)
            
            if position == target:
                return path
            
            for neighbor in self.get_neighbors(position, observation, env):
                if neighbor in closed_set or not self.is_valid_position(neighbor, observation, env):
                    continue
                
                g_score = len(path)  # å®é™…è·ç¦»
                h_score = self.heuristic(neighbor, target)  # å¯å‘å¼è·ç¦»
                f_score = g_score + h_score
                
                heapq.heappush(open_set, (f_score, neighbor, path + [neighbor]))
        
        return []
    
    def heuristic(self, pos1, pos2):
        """å¯å‘å¼å‡½æ•°ï¼šæ›¼å“ˆé¡¿è·ç¦»"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
```

## ğŸ§ª æµ‹è¯•å’Œè°ƒä¼˜

### æ€§èƒ½æµ‹è¯•

```bash
# æµ‹è¯•åŸºç¡€AIæ€§èƒ½
python evaluate_ai.py --agents improved_random rule_based greedy_snake --benchmark --games 100

# æ¯”è¾ƒä¸åŒAI
python evaluate_ai.py --agents random improved_random rule_based --compare --games 50

# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
python evaluate_ai.py --agents rule_based --benchmark --games 200 --plot --save rule_based_results.json
```

### è°ƒä¼˜å»ºè®®

1. **å‚æ•°è°ƒä¼˜**
   - è°ƒæ•´è¯„ä¼°å‡½æ•°ä¸­å„é¡¹çš„æƒé‡
   - ä¿®æ”¹æœç´¢æ·±åº¦å’Œå®½åº¦
   - ä¼˜åŒ–è§„åˆ™ä¼˜å…ˆçº§

2. **æ€§èƒ½ä¼˜åŒ–**
   - æ·»åŠ ç¼“å­˜é¿å…é‡å¤è®¡ç®—
   - ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
   - é™åˆ¶æœç´¢æ—¶é—´

3. **ç­–ç•¥æ”¹è¿›**
   - åˆ†æå¤±è´¥çš„æ¸¸æˆæ‰¾å‡ºé—®é¢˜
   - æ·»åŠ æ–°çš„è§„åˆ™æˆ–è¯„ä¼°é¡¹
   - ç»“åˆå¤šç§ç®—æ³•çš„ä¼˜åŠ¿

## ğŸ’¡ å®ç°æŠ€å·§

### 1. è°ƒè¯•æŠ€å·§
```python
def get_action(self, observation, env):
    action = self.calculate_best_action(observation, env)
    
    # è°ƒè¯•è¾“å‡º
    if self.debug:
        print(f"Player {self.player_id} chose action {action}")
        print(f"Board state: {observation['board']}")
    
    return action
```

### 2. å¼‚å¸¸å¤„ç†
```python
def get_action(self, observation, env):
    try:
        return self.smart_action(observation, env)
    except Exception as e:
        print(f"Error in {self.name}: {e}")
        # é™çº§åˆ°ç®€å•ç­–ç•¥
        return random.choice(env.get_valid_actions())
```

### 3. æ—¶é—´æ§åˆ¶
```python
import time

def get_action(self, observation, env):
    start_time = time.time()
    timeout = 5.0  # 5ç§’è¶…æ—¶
    
    for depth in range(1, 10):
        if time.time() - start_time > timeout:
            break
        
        action = self.search_with_depth(observation, env, depth)
    
    return action
```

## ğŸš€ 5. å¼ºåŒ–å­¦ä¹ AIï¼ˆæŒ‘æˆ˜é€‰åšï¼‰

### åŸºæœ¬æ€è·¯
- é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥
- ä½¿ç”¨Q-learningæˆ–ç®€åŒ–çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®å’Œæ—¶é—´

### Q-Learningå®ç°ç¤ºä¾‹

```python
import pickle
import numpy as np

class QLearningBot(BaseAgent):
    def __init__(self, name="QLearningBot", player_id=1):
        super().__init__(name, player_id)
        self.q_table = {}
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.epsilon = 0.1  # æ¢ç´¢ç‡
        self.training = True
        
    def get_action(self, observation, env):
        state = self.observation_to_state(observation)
        valid_actions = env.get_valid_actions()
        
        # Îµ-è´ªå¿ƒç­–ç•¥
        if self.training and random.random() < self.epsilon:
            return random.choice(valid_actions)
        
        # é€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
        q_values = {}
        for action in valid_actions:
            q_values[action] = self.q_table.get((state, action), 0.0)
        
        best_action = max(q_values, key=q_values.get)
        return best_action
    
    def observation_to_state(self, observation):
        """å°†è§‚å¯Ÿè½¬æ¢ä¸ºçŠ¶æ€è¡¨ç¤º"""
        # ç®€åŒ–çŠ¶æ€è¡¨ç¤ºï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
        if 'board' in observation:
            # äº”å­æ£‹ï¼šä½¿ç”¨æ£‹ç›˜çš„ç®€åŒ–è¡¨ç¤º
            board = observation['board']
            return tuple(board.flatten())
        else:
            # è´ªåƒè›‡ï¼šä½¿ç”¨å…³é”®ä¿¡æ¯
            snake = observation.get(f'snake{self.player_id}', [])
            foods = observation.get('foods', [])
            return (tuple(snake[:3]), tuple(foods[:2]))  # ç®€åŒ–è¡¨ç¤º
    
    def update_q_value(self, state, action, reward, next_state):
        """æ›´æ–°Qå€¼"""
        current_q = self.q_table.get((state, action), 0.0)
        
        # è·å–ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
        next_actions = self.get_possible_actions(next_state)
        max_next_q = 0.0
        if next_actions:
            max_next_q = max([self.q_table.get((next_state, a), 0.0) 
                             for a in next_actions])
        
        # Q-learningæ›´æ–°å…¬å¼
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[(state, action)] = new_q
    
    def train(self, env, episodes=1000):
        """è®­ç»ƒQ-learningæ™ºèƒ½ä½“"""
        print(f"å¼€å§‹è®­ç»ƒ {episodes} è½®...")
        
        for episode in range(episodes):
            observation, _ = env.reset()
            state = self.observation_to_state(observation)
            total_reward = 0
            
            while not env.is_terminal():
                action = self.get_action(observation, env)
                next_obs, reward, done, info = env.step(action)
                next_state = self.observation_to_state(next_obs)
                
                # æ›´æ–°Qå€¼
                self.update_q_value(state, action, reward, next_state)
                
                state = next_state
                observation = next_obs
                total_reward += reward
                
                if done:
                    break
            
            # è¡°å‡æ¢ç´¢ç‡
            if episode % 100 == 0:
                self.epsilon = max(0.01, self.epsilon * 0.995)
                print(f"Episode {episode}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.3f}")
    
    def save_model(self, filename):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filename, 'wb') as f:
            pickle.dump(self.q_table, f)
    
    def load_model(self, filename):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            with open(filename, 'rb') as f:
                self.q_table = pickle.load(f)
            self.training = False
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {filename}")
        except FileNotFoundError:
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
```

### è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
def train_q_learning_bot():
    """è®­ç»ƒQ-learningæ™ºèƒ½ä½“"""
    from games.gomoku import GomokuEnv
    from agents import RandomBot
    
    env = GomokuEnv(board_size=9, win_length=5)
    q_bot = QLearningBot("Q-Learning Bot", 1)
    random_bot = RandomBot("Random Bot", 2)
    
    # è®­ç»ƒé˜¶æ®µ
    q_bot.train(env, episodes=5000)
    
    # ä¿å­˜æ¨¡å‹
    q_bot.save_model("q_learning_gomoku.pkl")
    
    # æµ‹è¯•é˜¶æ®µ
    q_bot.training = False
    q_bot.epsilon = 0  # å…³é—­æ¢ç´¢
    
    # è¯„ä¼°æ€§èƒ½
    wins = 0
    test_games = 100
    for _ in range(test_games):
        observation, _ = env.reset()
        while not env.is_terminal():
            if env.game.current_player == 1:
                action = q_bot.get_action(observation, env)
            else:
                action = random_bot.get_action(observation, env)
            observation, _, done, _ = env.step(action)
            if done:
                break
        
        if env.get_winner() == 1:
            wins += 1
    
    print(f"è®­ç»ƒåèƒœç‡: {wins/test_games:.2%}")
```

## ğŸ¤– 6. å¤§è¯­è¨€æ¨¡å‹AIï¼ˆåˆ›æ–°é€‰åšï¼‰

### åŸºæœ¬æ€è·¯
- å°†æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
- ä½¿ç”¨æç¤ºå·¥ç¨‹æŒ‡å¯¼å¤§æ¨¡å‹åšå†³ç­–
- å¯ä»¥ä½¿ç”¨APIæˆ–æœ¬åœ°æ¨¡å‹

### åŸºç¡€å®ç°æ¡†æ¶

```python
import json
import requests
from typing import Optional

class LLMBot(BaseAgent):
    def __init__(self, name="LLMBot", player_id=1, model_type="openai"):
        super().__init__(name, player_id)
        self.model_type = model_type
        self.api_key = None  # è®¾ç½®ä½ çš„APIå¯†é’¥
        
    def get_action(self, observation, env):
        try:
            # è½¬æ¢æ¸¸æˆçŠ¶æ€ä¸ºæ–‡å­—æè¿°
            game_description = self.observation_to_text(observation, env)
            
            # æ„å»ºæç¤ºè¯
            prompt = self.build_prompt(game_description, env)
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.call_llm(prompt)
            
            # è§£æå›å¤è·å–åŠ¨ä½œ
            action = self.parse_action(response, env)
            
            if action and action in env.get_valid_actions():
                return action
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™ç­–ç•¥
                return self.fallback_strategy(observation, env)
                
        except Exception as e:
            print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return self.fallback_strategy(observation, env)
    
    def observation_to_text(self, observation, env):
        """å°†æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸ºæ–‡å­—æè¿°"""
        if hasattr(env, 'board_size'):  # äº”å­æ£‹
            board = observation['board']
            description = f"æ£‹ç›˜å¤§å°: {board.shape[0]}x{board.shape[1]}\n"
            
            # æè¿°æ£‹ç›˜çŠ¶æ€
            description += "å½“å‰æ£‹ç›˜çŠ¶æ€:\n"
            for i in range(board.shape[0]):
                row_desc = ""
                for j in range(board.shape[1]):
                    if board[i, j] == 0:
                        row_desc += "Â·"
                    elif board[i, j] == 1:
                        row_desc += "â—"
                    else:
                        row_desc += "â—‹"
                description += f"{i:2d}|{row_desc}|\n"
            
            # æ·»åŠ åˆ—æ ‡å·
            col_numbers = "  " + "".join([str(i%10) for i in range(board.shape[1])])
            description += col_numbers + "\n"
            
            return description
            
        else:  # è´ªåƒè›‡
            snake1 = observation.get('snake1', [])
            snake2 = observation.get('snake2', [])
            foods = observation.get('foods', [])
            
            description = f"è´ªåƒè›‡æ¸¸æˆçŠ¶æ€:\n"
            description += f"ä½ çš„è›‡(ç©å®¶{self.player_id}): {snake1 if self.player_id == 1 else snake2}\n"
            description += f"å¯¹æ‰‹çš„è›‡: {snake2 if self.player_id == 1 else snake1}\n"
            description += f"é£Ÿç‰©ä½ç½®: {foods}\n"
            
            return description
    
    def build_prompt(self, game_description, env):
        """æ„å»ºç»™å¤§æ¨¡å‹çš„æç¤ºè¯"""
        game_name = env.__class__.__name__.replace('Env', '')
        valid_actions = env.get_valid_actions()
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{game_name}æ¸¸æˆAIã€‚

{game_description}

ä½ æ˜¯ç©å®¶{self.player_id}ã€‚è¯·åˆ†æå½“å‰å±€åŠ¿å¹¶é€‰æ‹©æœ€ä½³åŠ¨ä½œã€‚

å¯é€‰åŠ¨ä½œ: {valid_actions}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤:
åˆ†æ: [ä½ çš„åˆ†æ]
åŠ¨ä½œ: (row, col)

æ³¨æ„:
1. åªèƒ½ä»å¯é€‰åŠ¨ä½œä¸­é€‰æ‹©
2. åŠ¨ä½œæ ¼å¼å¿…é¡»æ˜¯(row, col)çš„å½¢å¼
3. ä¼˜å…ˆè€ƒè™‘è·èƒœæœºä¼š
4. å…¶æ¬¡è€ƒè™‘é˜»æ­¢å¯¹æ‰‹è·èƒœ
5. é€‰æ‹©æˆ˜ç•¥ä½ç½®
"""
        return prompt
    
    def call_llm(self, prompt):
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
        if self.model_type == "openai":
            return self.call_openai(prompt)
        elif self.model_type == "ollama":
            return self.call_ollama(prompt)
        else:
            # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨è§„åˆ™æ¨¡æ‹Ÿå¤§æ¨¡å‹å›å¤
            return self.simulate_llm_response(prompt)
    
    def call_openai(self, prompt):
        """è°ƒç”¨OpenAI API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
            "max_tokens": 200
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        
        return response.json()["choices"][0]["message"]["content"]
    
    def call_ollama(self, prompt):
        """è°ƒç”¨æœ¬åœ°Ollamaæ¨¡å‹"""
        data = {
            "model": "llama2",  # æˆ–å…¶ä»–æ¨¡å‹
            "prompt": prompt,
            "stream": False
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=data,
            timeout=30
        )
        
        return response.json()["response"]
    
    def simulate_llm_response(self, prompt):
        """æ¨¡æ‹Ÿå¤§æ¨¡å‹å›å¤ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„è§„åˆ™æ¥æ¨¡æ‹Ÿå¤§æ¨¡å‹çš„å›å¤
        return "åˆ†æ: é€‰æ‹©ä¸­å¿ƒä½ç½®\nåŠ¨ä½œ: (4, 4)"
    
    def parse_action(self, response, env):
        """ä»å¤§æ¨¡å‹å›å¤ä¸­è§£æåŠ¨ä½œ"""
        import re
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åŠ¨ä½œ
        pattern = r'åŠ¨ä½œ[:ï¼š]\s*\((\d+),\s*(\d+)\)'
        match = re.search(pattern, response)
        
        if match:
            row, col = int(match.group(1)), int(match.group(2))
            return (row, col)
        
        # å°è¯•å…¶ä»–æ ¼å¼
        pattern2 = r'\((\d+),\s*(\d+)\)'
        matches = re.findall(pattern2, response)
        if matches:
            row, col = int(matches[-1][0]), int(matches[-1][1])
            return (row, col)
        
        return None
    
    def fallback_strategy(self, observation, env):
        """é™çº§ç­–ç•¥"""
        # å¦‚æœLLMå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§„åˆ™
        valid_actions = env.get_valid_actions()
        if valid_actions:
            # ä¼˜å…ˆé€‰æ‹©ä¸­å¿ƒä½ç½®
            if hasattr(env, 'board_size'):
                center = env.board_size // 2
                for action in valid_actions:
                    row, col = action
                    if abs(row - center) <= 1 and abs(col - center) <= 1:
                        return action
            return random.choice(valid_actions)
        return None
```

### ä½¿ç”¨ç¤ºä¾‹

```python
def test_llm_bot():
    """æµ‹è¯•LLM Bot"""
    from games.gomoku import GomokuEnv
    
    # åˆ›å»ºç¯å¢ƒå’ŒAI
    env = GomokuEnv(board_size=9, win_length=5)
    
    # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰
    llm_bot = LLMBot("LLM Bot", 1, model_type="simulate")
    
    # å¦‚æœè¦ä½¿ç”¨çœŸå®APIï¼Œéœ€è¦è®¾ç½®å¯†é’¥
    # llm_bot.api_key = "your-openai-api-key"
    # llm_bot.model_type = "openai"
    
    observation, _ = env.reset()
    action = llm_bot.get_action(observation, env)
    print(f"LLMé€‰æ‹©çš„åŠ¨ä½œ: {action}")
```

### ä½¿ç”¨å»ºè®®

1. **APIè´¹ç”¨æ§åˆ¶**: è®¾ç½®è¯·æ±‚é™åˆ¶ï¼Œé¿å…è¿‡åº¦è°ƒç”¨
2. **æç¤ºè¯ä¼˜åŒ–**: å¤šæµ‹è¯•ä¸åŒçš„æç¤ºè¯æ ¼å¼
3. **é”™è¯¯å¤„ç†**: å¿…é¡»æœ‰é™çº§ç­–ç•¥
4. **æœ¬åœ°æ¨¡å‹**: å¯ä»¥ä½¿ç”¨Ollamaç­‰å…è´¹æœ¬åœ°æ¨¡å‹

## ğŸ“ˆ è¿›é˜¶æ–¹å‘

å®ŒæˆåŸºç¡€AIåï¼Œå¯ä»¥å°è¯•ï¼š

1. **ç»„åˆç­–ç•¥**: ç»“åˆå¤šç§ç®—æ³•çš„ä¼˜åŠ¿
2. **è‡ªé€‚åº”AI**: æ ¹æ®å¯¹æ‰‹é£æ ¼è°ƒæ•´ç­–ç•¥
3. **å¼ºåŒ–å­¦ä¹ **: Q-learningã€Actor-Criticç­‰
4. **ç¥ç»ç½‘ç»œ**: æ·±åº¦å­¦ä¹ æ–¹æ³•
5. **å¤šæ™ºèƒ½ä½“å­¦ä¹ **: æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œä¸ç«äº‰

è®°ä½ï¼šå¥½çš„AIä¸æ˜¯æœ€å¤æ‚çš„ç®—æ³•ï¼Œè€Œæ˜¯æœ€é€‚åˆç‰¹å®šæ¸¸æˆçš„ç­–ç•¥ï¼ 